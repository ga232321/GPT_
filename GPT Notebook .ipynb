{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf8c00a",
   "metadata": {},
   "source": [
    "# Вариант парсинга данных для обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import threading\n",
    "\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "\n",
    "client = OpenAI(api_key='')\n",
    "\n",
    "def gpt_get():\n",
    "    for i in range(1000):\n",
    "        prompt = \"\"\"I want you to ask yourself \n",
    "        a random question or a promt on any topic and then answer on it, length of you answer should be not less then 500 words\"\"\"\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "          model=\"gpt-3.5-turbo\",# gpt4\n",
    "          messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"You a helpfull assistent \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "          ]\n",
    "        )\n",
    "\n",
    "        a = completion.choices[0].message.content\n",
    "\n",
    "        answers.append(a[a.find('nswer')+6:])\n",
    "        questions.append(a[9:a.find('nswer')-1])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    thread = threading.Thread(target=gpt_get)\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "df = pd.DataFrame({\"Questions\":questions,\"Answers\":answers})\n",
    "\n",
    "df.to_pickle('promt_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142900ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dad4000",
   "metadata": {},
   "source": [
    "# Архитектура сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8cf1e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca3389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinformerAttention(nn.Module):\n",
    "    def __init__(self, dim: int, seq_len: int, heads: int, k: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert dim % heads == 0\n",
    "\n",
    "        # Stores constant values.\n",
    "        self.seq_len = seq_len\n",
    "        self.k = k\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim // heads\n",
    "\n",
    "        # Similar to the original transformer implementation, but with two\n",
    "        # extra parameters for projecting from the full sequence length.\n",
    "        self.to_q = nn.Linear(dim, dim, bias=False)\n",
    "        self.to_k = nn.Linear(dim, dim, bias=False)\n",
    "        self.to_v = nn.Linear(dim, dim, bias=False)\n",
    "        self.proj_k = nn.Parameter(torch.randn(seq_len, k))\n",
    "        self.proj_v = nn.Parameter(torch.randn(seq_len, k))\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (b, n, d), d_h, h, k = x.shape, self.dim_head, self.heads, self.k\n",
    "\n",
    "       \n",
    "        queries = self.to_q(x)\n",
    "        keys = self.to_k(x)\n",
    "        values = self.to_v(x)\n",
    "\n",
    "        \n",
    "        keys = torch.einsum(\"bnd,nk->bkd\", keys, self.proj_k)\n",
    "        values = torch.einsum(\"bnd,nk->bkd\", values, self.proj_v)\n",
    "\n",
    "        \n",
    "        queries = queries.reshape(b, n, h, -1).transpose(1, 2)  # (B, N, D) -> (B, H, N, D // H)\n",
    "        keys = keys.reshape(b, k, h, -1).transpose(1, 2)  # (B, K, D) -> (B, H, K, D // H)\n",
    "        values = values.reshape(b, k, h, -1).transpose(1, 2)  # (B, K, D) -> (B, H, K, D // H)\n",
    "\n",
    "        out,attention = scale_dot_product(queries, keys, values)\n",
    "\n",
    "        \n",
    "        out = out.transpose(1, 2).reshape(b, n, -1)  # (B, H, N, D // H) -> (B, N, D)\n",
    "\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a361a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dot_product(q,k,v):\n",
    "    L, S = q.size(-2), k.size(-2)\n",
    "    d_k = q.size(-1)\n",
    "    scaled = q@k.transpose(-1,-2)*d_k**(-0.5)# 200x512x512\n",
    "\n",
    "    scaled = scaled.masked_fill(torch.tril(torch.ones(L, S)).to(device) == 0, float('-inf'))\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = attention@v\n",
    "    return values,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fb12775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "\n",
    "def scale_dot_product(q,k,v):\n",
    "    L, S = q.size(-2), k.size(-2)\n",
    "    d_k = q.size(-1)\n",
    "    scaled = q@k.transpose(-1,-2)*d_k**(-0.5)# 200x512x512\n",
    "    #print(scaled.shape)\n",
    "    scaled = scaled.masked_fill(torch.tril(torch.ones(L, S)).to(device) == 0, float('-inf'))\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = attention@v\n",
    "    return values,attention\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_model, ffn_hidden , drop_prob):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model,ffn_hidden)\n",
    "        self.dropout1 = nn.Dropout(drop_prob)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear2 = nn.Linear(ffn_hidden, d_model)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.dropout1(self.gelu(self.linear1(x)))\n",
    "        return self.linear2(x)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model//n_heads\n",
    "        self.qkv = nn.Linear(d_model, d_model*3)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        batch_size, sequence_length, d_model = x.size()\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(batch_size,sequence_length,self.n_heads, self.head_dim*3).permute(0,2,1,3)\n",
    "        q,k,v = qkv.chunk(3,dim=-1)#\n",
    "        values, attention = scale_dot_product(q,k,v)\n",
    "        \n",
    "        values = values.permute(0,2,1,3).reshape(batch_size,sequence_length,d_model)\n",
    "        return self.linear(values)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,d_model, n_heads,drop_prob,max_len,linear):\n",
    "        super().__init__()\n",
    "        if linear:\n",
    "            self.attention = LinformerAttention(d_model, seq_len = max_len, heads = n_heads, k=max_len//2)\n",
    "        else:\n",
    "            self.attention = MultiHeadAttention(d_model,n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, ffn_hidden=d_model*3, drop_prob=drop_prob)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x):# 64x200x512\n",
    "\n",
    "        x = x + self.attention(self.norm1(self.dropout(x)))\n",
    "        x = x + self.attention(self.norm1(self.dropout(x)))\n",
    "        x = x + self.ffn(self.norm2(self.dropout(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eee4cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "class ScratchTransformer(nn.Module):\n",
    "    def __init__(self,d_model,n_layers,n_heads,dropout,max_len,linear=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pos_vect = torch.LongTensor([i for i in range(max_len)]).to(device)\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(tokenizer.vocab_size, d_model, padding_idx=0).to(device)\n",
    "        self.pos_embedding_table = nn.Embedding(max_len, d_model).to(device)\n",
    "        self.blocks = nn.Sequential(*[Block(d_model, n_heads=n_heads,drop_prob=dropout,max_len= max_len,linear=linear) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model) # final layer norm\n",
    "        self.lm_head = nn.Linear(d_model, tokenizer.vocab_size)\n",
    "                                          \n",
    "                                                    \n",
    "    def forward(self,x):\n",
    "        \n",
    "\n",
    "        x = self.token_embedding_table(x) +  self.pos_embedding_table(self.pos_vect)\n",
    "        \n",
    "        def run_checkpoint(x):\n",
    "            x = self.blocks(x)\n",
    "            return x\n",
    "        \n",
    "        x = checkpoint.checkpoint(run_checkpoint, x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        x = self.lm_head(x)\n",
    "        b,s,d = x.shape\n",
    "        return x.view(b*s,d)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081b2b9",
   "metadata": {},
   "source": [
    "# Wiki-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d5b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ga232\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1454: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_dataset = load_dataset('wikipedia','20220301.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3f055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05affd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e685ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast as BertTokenizer\n",
    "\n",
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ff1ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = ScratchTransformer(64,8,8,0.1,500,True)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfcf5282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model(torch.tensor([[i for i in range(500)]]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f27ad837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_func(data):\n",
    "    chunks = []\n",
    "    \n",
    "    dataa = data['text'].split()\n",
    "    for i in range((len(dataa)//350) -1):\n",
    "        chunks.append(dataa[i*350:(i+1)*350:])\n",
    "        \n",
    "    chunks.append(dataa[(len(dataa)//350)*350:])\n",
    "    chunks = list(map(lambda x: ' '.join(x),chunks))\n",
    "\n",
    "    func = lambda x:tokenizer.encode_plus(\n",
    "      x,\n",
    "      add_special_tokens=True,\n",
    "      max_length=500,\n",
    "      return_token_type_ids=False,\n",
    "      padding=\"max_length\",\n",
    "      return_attention_mask=True,\n",
    "      #return_tensors='pt',\n",
    "    )[\"input_ids\"][:500] #[0][:500] \n",
    "\n",
    "    return list(map(func,chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57618f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                 lr = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e54081e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScratchTransformer(\n",
       "  (token_embedding_table): Embedding(28996, 64, padding_idx=0)\n",
       "  (pos_embedding_table): Embedding(500, 64)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (attention): LinformerAttention(\n",
       "        (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ffn): PositionwiseFeedForward(\n",
       "        (linear1): Linear(in_features=64, out_features=192, bias=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (linear2): Linear(in_features=192, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (attention): LinformerAttention(\n",
       "        (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ffn): PositionwiseFeedForward(\n",
       "        (linear1): Linear(in_features=64, out_features=192, bias=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (linear2): Linear(in_features=192, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (attention): LinformerAttention(\n",
       "        (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ffn): PositionwiseFeedForward(\n",
       "        (linear1): Linear(in_features=64, out_features=192, bias=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (linear2): Linear(in_features=192, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (attention): LinformerAttention(\n",
       "        (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ffn): PositionwiseFeedForward(\n",
       "        (linear1): Linear(in_features=64, out_features=192, bias=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (linear2): Linear(in_features=192, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (attention): LinformerAttention(\n",
       "        (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ffn): PositionwiseFeedForward(\n",
       "        (linear1): Linear(in_features=64, out_features=192, bias=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (linear2): Linear(in_features=192, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (attention): LinformerAttention(\n",
       "        (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ffn): PositionwiseFeedForward(\n",
       "        (linear1): Linear(in_features=64, out_features=192, bias=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (linear2): Linear(in_features=192, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): Block(\n",
       "      (attention): LinformerAttention(\n",
       "        (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ffn): PositionwiseFeedForward(\n",
       "        (linear1): Linear(in_features=64, out_features=192, bias=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (linear2): Linear(in_features=192, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): Block(\n",
       "      (attention): LinformerAttention(\n",
       "        (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ffn): PositionwiseFeedForward(\n",
       "        (linear1): Linear(in_features=64, out_features=192, bias=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (linear2): Linear(in_features=192, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=64, out_features=28996, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a135f163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35698e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 10\n",
    "num_batches = 0\n",
    "\n",
    "for n,i in enumerate(tqdm(wiki_dataset['train'])):\n",
    "    chunks = chunk_func(i)\n",
    "\n",
    "    x,y = torch.tensor(chunks), torch.cat((torch.tensor(chunks)[:,1:],torch.LongTensor([[0] for i in range(len(chunks))])),dim=1)\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "    # evaluate the loss\n",
    "    pred = model(x)\n",
    "    loss = F.cross_entropy(pred, y.reshape(-1))\n",
    "\n",
    "\n",
    "    loss = loss / gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "    num_batches+=1\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if n%100==0:\n",
    "        clear_output()\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93b30e6",
   "metadata": {},
   "source": [
    "# Дообучение итоговой модели на promt'ах GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "a = load_dataset(\"vicgalle/alpaca-gpt4\", 'default',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c6300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(a['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"instruction\"] = data[\"instruction\"].apply(lambda x:tokenizer.encode_plus(\n",
    "  x,\n",
    "  add_special_tokens=True,\n",
    "  max_length=500,\n",
    "  return_token_type_ids=False,\n",
    "  padding=\"max_length\",\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")[\"input_ids\"][0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbfa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['output'] = data[\"output\"].apply(lambda x:tokenizer.encode_plus(\n",
    "  x,\n",
    "  add_special_tokens=True,\n",
    "  max_length=500,\n",
    "  return_token_type_ids=False,\n",
    "  padding=\"max_length\",\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")[\"input_ids\"][0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6d9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        #self.data = data.reset_index().drop(\"index\",axis=1).to_numpy()\n",
    "        self.X = data[\"instruction\"].to_numpy()\n",
    "        self.y = data['output'].to_numpy()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return  self.X[idx], self.y[idx],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98de105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(data,train_size = 0.8,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df)\n",
    "test_dataset = TextDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258910c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=32,)#pin_memory = True)\n",
    "valid_loader = DataLoader(test_dataset,batch_size=64,)#pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b962659",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                 lr = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda7cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "loss_sum = 0\n",
    "gradient_accumulation_steps = 2\n",
    "for epoch in tqdm(range(100)):\n",
    "    for i,(x,y) in enumerate(tqdm(train_loader)):\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = F.cross_entropy(pred, y.reshape(-1))\n",
    "\n",
    "\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            break\n",
    "        if i%100==0:\n",
    "            clear_output()\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080b3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
